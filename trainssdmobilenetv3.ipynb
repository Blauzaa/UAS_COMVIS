{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c06081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# PRASYARAT DAN IMPORT\n",
    "# ===================================================================\n",
    "import os\n",
    "os.environ['MMENGINE_DISABLE_WEIGHTS_ONLY_LOAD'] = '1'\n",
    "\n",
    "!pip install timm -q\n",
    "!pip install -U albumentations --no-binary qudida,albumentations -q\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmengine.registry import MODELS as MMENGINE_MODELS\n",
    "from mmengine.dataset import Compose\n",
    "\n",
    "from mmdet.utils import register_all_modules\n",
    "from mmdet.registry import MODELS\n",
    "from mmdet.models.dense_heads import SSDHead\n",
    "from mmdet.visualization import DetLocalVisualizer\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b629de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 1: KONFIGURASI UTAMA\n",
    "# ===================================================================\n",
    "# --- Hyperparameters untuk Eksperimen ---\n",
    "PROJECT_NAME = \"BreadDetector_FPN_Mosaic_AdamW_300epoch_lr0.0001_bs4\" \n",
    "DATASET_ROOT = 'dataset' # Ganti dengan path ke folder dataset Anda\n",
    "CLASSES_TO_USE = [\n",
    "    'baguette', 'cornbread', 'croissant', 'ensaymada', 'flatbread',\n",
    "    'sourdough', 'wheat-bread', 'white-bread', 'whole-grain-bread', 'pandesal'\n",
    "]\n",
    "NUM_CLASSES = len(CLASSES_TO_USE)\n",
    "\n",
    "# --- Parameter Training ---\n",
    "BASE_LR = 0.0001\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 300 # Tingkatkan jumlah epoch untuk hasil yang lebih baik\n",
    "IMG_SIZE = (512, 512)\n",
    "\n",
    "# --- Direktori Kerja ---\n",
    "WORK_DIR = f\"outputs/{PROJECT_NAME}\"\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Project Name: {PROJECT_NAME}\")\n",
    "print(f\"Work Directory: {WORK_DIR}\")\n",
    "print(f\"Classes ({NUM_CLASSES}): {CLASSES_TO_USE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9262a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 2: DEFINISI KELAS KUSTOM (UNTUK MENANGANI ARGUMEN LOSS)\n",
    "# ===================================================================\n",
    "from mmdet.models.dense_heads import SSDHead\n",
    "from mmdet.registry import MODELS\n",
    "from mmengine.registry import MODELS as MMENGINE_MODELS\n",
    "import torch\n",
    "\n",
    "@MODELS.register_module(force=True)\n",
    "class CustomSSDHead(SSDHead):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Logika ini bisa menangani argumen loss_cls dan loss_bbox\n",
    "        loss_cls_cfg = kwargs.pop('loss_cls', None)\n",
    "        loss_bbox_cfg = kwargs.pop('loss_bbox', None)\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Jika didefinisikan, buat objek loss.\n",
    "        if loss_cls_cfg:\n",
    "            self.loss_cls = MODELS.build(loss_cls_cfg)\n",
    "        if loss_bbox_cfg:\n",
    "            self.loss_bbox = MODELS.build(loss_bbox_cfg)\n",
    "\n",
    "print(\"CustomSSDHead (untuk menangani loss) berhasil didaftarkan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 3: FUNGSI FILTER ANOTASI (Tetap sama)\n",
    "# ===================================================================\n",
    "def filter_coco_annotations(original_ann_file, new_ann_file, classes_to_keep):\n",
    "    if os.path.exists(new_ann_file):\n",
    "        print(f\"File anotasi sudah ada: {new_ann_file}. Melewati pembuatan ulang.\")\n",
    "        return\n",
    "    with open(original_ann_file, 'r') as f: coco_data = json.load(f)\n",
    "    original_categories = {cat['name']: cat['id'] for cat in coco_data['categories']}\n",
    "    new_categories, old_id_to_new_id, new_cat_id = [], {}, 0\n",
    "    for cat_name in classes_to_keep:\n",
    "        if cat_name in original_categories:\n",
    "            new_categories.append({'id': new_cat_id, 'name': cat_name, 'supercategory': 'object'})\n",
    "            old_id_to_new_id[original_categories[cat_name]] = new_cat_id\n",
    "            new_cat_id += 1\n",
    "    new_annotations, image_ids_with_annotations = [], set()\n",
    "    for ann in coco_data['annotations']:\n",
    "        if ann.get('iscrowd', 0) == 0 and ann['category_id'] in old_id_to_new_id:\n",
    "            ann['category_id'] = old_id_to_new_id[ann['category_id']]\n",
    "            new_annotations.append(ann)\n",
    "            image_ids_with_annotations.add(ann['image_id'])\n",
    "    new_images = [img for img in coco_data['images'] if img['id'] in image_ids_with_annotations]\n",
    "    new_coco_data = {'images': new_images, 'annotations': new_annotations, 'categories': new_categories, 'info': coco_data.get('info', {}), 'licenses': coco_data.get('licenses', [])}\n",
    "    os.makedirs(os.path.dirname(new_ann_file), exist_ok=True)\n",
    "    with open(new_ann_file, 'w') as f: json.dump(new_coco_data, f, indent=4)\n",
    "    print(f\"File anotasi baru disimpan di: {new_ann_file}\")\n",
    "\n",
    "original_train_ann, filtered_train_ann = os.path.join(DATASET_ROOT, 'train', '_annotations.coco.json'), os.path.join(DATASET_ROOT, 'train', 'filtered_annotations.coco.json')\n",
    "original_valid_ann, filtered_valid_ann = os.path.join(DATASET_ROOT, 'valid', '_annotations.coco.json'), os.path.join(DATASET_ROOT, 'valid', 'filtered_annotations.coco.json')\n",
    "filter_coco_annotations(original_train_ann, filtered_train_ann, CLASSES_TO_USE)\n",
    "filter_coco_annotations(original_valid_ann, filtered_valid_ann, CLASSES_TO_USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3cef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 4: KONFIGURASI PENINGKATAN PERFORMA (MOSAIC + AUGMENTASI KUAT)\n",
    "# ===================================================================\n",
    "cfg = Config()\n",
    "register_all_modules(init_default_scope=False)\n",
    "cfg.custom_imports = dict(imports=['mmpretrain.models.backbones.timm_backbone'], allow_failed_imports=False)\n",
    "\n",
    "# --- KEMBALIKAN AUGMENTASI UNTUK MENCEGAH OVERFITTING ---\n",
    "albu_train_transforms = [\n",
    "    dict(type='RandomBrightnessContrast', brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    dict(type='GaussNoise', variance_limit=(10.0, 50.0), p=0.3),\n",
    "    dict(type='OneOf',\n",
    "         transforms=[\n",
    "             dict(type='Blur', blur_limit=3, p=1.0),\n",
    "             dict(type='MotionBlur', blur_limit=3, p=1.0)\n",
    "         ], p=0.3),\n",
    "]\n",
    "\n",
    "# --- PIPELINE BARU DENGAN MOSAIC DAN AUGMENTASI KUAT ---\n",
    "# 1. Pipeline dasar untuk memuat data sebelum augmentasi\n",
    "base_train_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=None),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "]\n",
    "\n",
    "# 2. Pipeline utama yang diawali dengan Mosaic\n",
    "train_pipeline = [\n",
    "    dict(type='Mosaic', img_scale=IMG_SIZE, pad_val=114.0),\n",
    "    dict(type='Resize', scale=IMG_SIZE, keep_ratio=True),\n",
    "    dict(type='Pad', size=IMG_SIZE, pad_val=dict(img=(114, 114, 114))),\n",
    "    dict(type='RandomFlip', prob=0.5),\n",
    "    dict(type='Albu',\n",
    "         transforms=albu_train_transforms,\n",
    "         bbox_params=dict(\n",
    "             type='BboxParams',\n",
    "             format='pascal_voc',\n",
    "             label_fields=['gt_bboxes_labels', 'gt_ignore_flags']),\n",
    "         keymap={'img': 'image', 'gt_bboxes': 'bboxes'}),\n",
    "    dict(type='PhotoMetricDistortion'),\n",
    "    dict(type='PackDetInputs')\n",
    "]\n",
    "\n",
    "# Pipeline validasi tetap sederhana\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=None),\n",
    "    dict(type='Resize', scale=IMG_SIZE, keep_ratio=True),\n",
    "    dict(type='Pad', size=IMG_SIZE, pad_val=dict(img=(114, 114, 114))),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(type='PackDetInputs', meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor'))\n",
    "]\n",
    "\n",
    "# Konfigurasi Visualizer (tidak berubah)\n",
    "cfg.visualizer = dict(\n",
    "    type='DetLocalVisualizer',\n",
    "    vis_backends=[dict(type='LocalVisBackend'), dict(type='TensorboardVisBackend')]\n",
    ")\n",
    "\n",
    "# --- KONFIGURASI MODEL FPN (TETAP SAMA, SUDAH STABIL) ---\n",
    "cfg.model = dict(\n",
    "    type='SingleStageDetector',\n",
    "    data_preprocessor=dict(type='DetDataPreprocessor', mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], bgr_to_rgb=True, pad_size_divisor=1),\n",
    "    backbone=dict(\n",
    "        type='mmpretrain.TIMMBackbone',\n",
    "        model_name='mobilenetv3_large_100',\n",
    "        features_only=True,\n",
    "        pretrained=True,\n",
    "        out_indices=(1, 2, 4)),\n",
    "    neck=dict(\n",
    "        type='FPN',\n",
    "        in_channels=[24, 40, 960],\n",
    "        out_channels=256,\n",
    "        start_level=1,\n",
    "        add_extra_convs='on_input',\n",
    "        num_outs=5),\n",
    "    bbox_head=dict(\n",
    "        type='CustomSSDHead',\n",
    "        in_channels=[256, 256, 256, 256, 256],\n",
    "        num_classes=NUM_CLASSES,\n",
    "        loss_cls=dict(\n",
    "            type='FocalLoss', use_sigmoid=True, gamma=2.0, alpha=0.25, loss_weight=1.0),\n",
    "        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.5),\n",
    "        anchor_generator=dict(\n",
    "            type='SSDAnchorGenerator',\n",
    "            scale_major=False,\n",
    "            input_size=IMG_SIZE[0],\n",
    "            strides=[8, 16, 32, 64, 128],\n",
    "            ratios=[[2], [2, 3], [2, 3], [2, 3], [2, 3]],\n",
    "            min_sizes=[30, 60, 111, 162, 213],\n",
    "            max_sizes=[60, 111, 162, 213, 264])),\n",
    "    train_cfg=dict(\n",
    "        assigner=dict(\n",
    "            type='MaxIoUAssigner', pos_iou_thr=0.5, neg_iou_thr=0.5, min_pos_iou=0.,\n",
    "            ignore_iof_thr=-1, gt_max_assign_all=False),\n",
    "        sampler=dict(type='PseudoSampler'),\n",
    "        smoothl1_beta=1.0,\n",
    "        allowed_border=-1,\n",
    "        pos_weight=-1,\n",
    "        neg_pos_ratio=3),\n",
    "    test_cfg=dict(\n",
    "        nms_pre=1000, min_bbox_size=0, score_thr=0.05,\n",
    "        nms=dict(type='nms', iou_threshold=0.5),\n",
    "        max_per_img=100))\n",
    "\n",
    "# --- DATALOADER DENGAN MultiImageMixDataset ---\n",
    "cfg.train_dataloader = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    dataset=dict(\n",
    "        type='MultiImageMixDataset',\n",
    "        dataset=dict(\n",
    "            type='CocoDataset',\n",
    "            data_root=DATASET_ROOT,\n",
    "            ann_file='train/filtered_annotations.coco.json',\n",
    "            data_prefix=dict(img='train/'),\n",
    "            metainfo=dict(classes=CLASSES_TO_USE),\n",
    "            filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
    "            pipeline=base_train_pipeline),\n",
    "        pipeline=train_pipeline))\n",
    "\n",
    "cfg.val_dataloader = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    drop_last=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type='CocoDataset',\n",
    "        data_root=DATASET_ROOT,\n",
    "        ann_file='valid/filtered_annotations.coco.json',\n",
    "        data_prefix=dict(img='valid/'),\n",
    "        metainfo=dict(classes=CLASSES_TO_USE),\n",
    "        test_mode=True,\n",
    "        pipeline=test_pipeline))\n",
    "\n",
    "# --- OPTIMIZER DAN SCHEDULER MODERN ---\n",
    "cfg.optim_wrapper = dict(\n",
    "    type='OptimWrapper',\n",
    "    optimizer=dict(type='AdamW', lr=BASE_LR, weight_decay=0.01))\n",
    "\n",
    "cfg.param_scheduler = [\n",
    "    dict(type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500),\n",
    "    dict(type='CosineAnnealingLR', T_max=EPOCHS, eta_min=BASE_LR * 0.1, by_epoch=True, begin=1, end=EPOCHS)\n",
    "]\n",
    "\n",
    "# --- SISA KONFIGURASI ---\n",
    "cfg.test_dataloader = cfg.val_dataloader\n",
    "cfg.val_evaluator = dict(type='CocoMetric', ann_file=os.path.join(DATASET_ROOT, 'valid/filtered_annotations.coco.json'), metric='bbox')\n",
    "cfg.test_evaluator = cfg.val_evaluator\n",
    "cfg.train_cfg = dict(type='EpochBasedTrainLoop', max_epochs=EPOCHS, val_interval=10) # Validasi setiap 10 epoch\n",
    "cfg.val_cfg = dict(type='ValLoop')\n",
    "cfg.test_cfg = dict(type='TestLoop')\n",
    "cfg.default_scope = 'mmdet'\n",
    "cfg.default_hooks = dict(\n",
    "    timer=dict(type='IterTimerHook'),\n",
    "    logger=dict(type='LoggerHook', interval=50),\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "    checkpoint=dict(type='CheckpointHook', interval=10, save_best='coco/bbox_mAP', rule='greater'),\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "    visualization=dict(type='DetVisualizationHook'))\n",
    "cfg.env_cfg = dict(cudnn_benchmark=True)\n",
    "cfg.log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)\n",
    "cfg.custom_hooks = []\n",
    "cfg.log_level = 'INFO'\n",
    "cfg.load_from = None\n",
    "cfg.resume = False\n",
    "cfg.work_dir = WORK_DIR\n",
    "\n",
    "print(\"\\nâœ… Konfigurasi PENINGKATAN PERFORMA (FPN+Mosaic+AdamW) telah dibuat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 5: MULAI TRAINING\n",
    "# ===================================================================\n",
    "runner = Runner.from_cfg(cfg)\n",
    "runner.train()\n",
    "print(f\"\\nProses training telah selesai. Semua output disimpan di: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71121d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 6: SETUP DIREKTORI ANALISIS DAN PENCARIAN PATH (VERSI FINAL ROBUST)\n",
    "# ===================================================================\n",
    "latest_run_dir = None\n",
    "best_checkpoint_path = None\n",
    "REPORT_DIR = None\n",
    "\n",
    "if os.path.exists(WORK_DIR):\n",
    "    # --- Cari subfolder run terbaru untuk LOGS ---\n",
    "    run_folders = [\n",
    "        d for d in os.listdir(WORK_DIR)\n",
    "        if os.path.isdir(os.path.join(WORK_DIR, d)) and d.replace('_', '').isdigit()\n",
    "    ]\n",
    "    if run_folders:\n",
    "        run_folders.sort()\n",
    "        latest_run_dir = os.path.join(WORK_DIR, run_folders[-1])\n",
    "        print(f\"--- Menganalisis direktori run (untuk log): {latest_run_dir} ---\")\n",
    "        \n",
    "        # Buat direktori laporan di dalam direktori run\n",
    "        REPORT_DIR = os.path.join(latest_run_dir, 'post_training_reports')\n",
    "        os.makedirs(REPORT_DIR, exist_ok=True)\n",
    "        print(f\"Laporan akan disimpan di: {REPORT_DIR}\")\n",
    "    else:\n",
    "        print(f\"--- PERINGATAN: Tidak ada subfolder run yang valid (untuk log) ditemukan di '{WORK_DIR}'.\")\n",
    "\n",
    "    # --- PERBAIKAN UTAMA: Cari checkpoint terbaik di WORK_DIR utama ---\n",
    "    # Ini terpisah dari pencarian subfolder log\n",
    "    best_ckpts = [f for f in os.listdir(WORK_DIR) if f.startswith('best_') and f.endswith('.pth')]\n",
    "    if best_ckpts:\n",
    "        # Ambil yang terbaru berdasarkan waktu modifikasi\n",
    "        best_ckpts.sort(key=lambda x: os.path.getmtime(os.path.join(WORK_DIR, x)))\n",
    "        best_checkpoint_path = os.path.join(WORK_DIR, best_ckpts[-1])\n",
    "        print(f\"Checkpoint terbaik ditemukan di direktori utama: {best_checkpoint_path}\")\n",
    "    else:\n",
    "        print(f\"PERINGATAN: Tidak ada checkpoint terbaik ('best_*.pth') yang ditemukan di {WORK_DIR}.\")\n",
    "\n",
    "# Jika salah satu tidak ditemukan, beri tahu pengguna\n",
    "if not latest_run_dir or not best_checkpoint_path:\n",
    "    print(\"\\n--- PERINGATAN: Analisis lengkap tidak dapat dilanjutkan karena log atau checkpoint terbaik tidak ditemukan. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 7: PLOTTING KURVA STABILITAS TRAINING (VERSI PALING ROBUST)\n",
    "# ===================================================================\n",
    "def create_stability_curve_by_epoch(run_dir, report_dir):\n",
    "    print(\"\\n>>> Membuat grafik stabilitas training (Loss vs. mAP)...\")\n",
    "    if not run_dir or not report_dir:\n",
    "        print(\"--- PERINGATAN: Direktori run atau laporan tidak valid. Melewati pembuatan grafik.\")\n",
    "        return\n",
    "\n",
    "    log_file = None\n",
    "    vis_data_path = os.path.join(run_dir, 'vis_data')\n",
    "\n",
    "    if os.path.exists(vis_data_path):\n",
    "        json_logs = [f for f in os.listdir(vis_data_path) if f.endswith('.json') and 'scalars' not in f]\n",
    "        if json_logs:\n",
    "            json_logs.sort(key=lambda x: os.path.getmtime(os.path.join(vis_data_path, x)))\n",
    "            log_file = os.path.join(vis_data_path, json_logs[-1])\n",
    "    \n",
    "    if not log_file or not os.path.exists(log_file):\n",
    "        print(f\"--- PERINGATAN: File log .json tidak ditemukan di '{vis_data_path}'. Grafik stabilitas tidak dapat dibuat.\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Membaca log dari: {log_file} ---\")\n",
    "    log_data = []\n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            try: log_data.append(pd.json_normalize(json.loads(line)))\n",
    "            except json.JSONDecodeError: continue\n",
    "\n",
    "    if not log_data:\n",
    "        print(\"--- GAGAL: Tidak ada data valid yang bisa dibaca dari file log.\")\n",
    "        return\n",
    "\n",
    "    log_df = pd.concat(log_data, ignore_index=True)\n",
    "\n",
    "    # Proses log training\n",
    "    train_log = log_df[log_df['loss'].notna()][['epoch', 'loss']].groupby('epoch').mean().reset_index()\n",
    "\n",
    "    # --- PERBAIKAN UTAMA: Isolasi kolom untuk menghindari duplikasi nama ---\n",
    "    val_log_raw = log_df[log_df['coco/bbox_mAP'].notna()].copy()\n",
    "    \n",
    "    if 'step' in val_log_raw.columns and 'coco/bbox_mAP' in val_log_raw.columns:\n",
    "        # 1. Buat DataFrame baru HANYA dengan kolom yang dibutuhkan\n",
    "        val_log_subset = val_log_raw[['step', 'coco/bbox_mAP']]\n",
    "        # 2. Ganti nama kolom di subset yang bersih ini\n",
    "        val_log_subset = val_log_subset.rename(columns={'step': 'epoch'})\n",
    "        # 3. Agregasi subset ini (dijamin tidak ada duplikat kolom 'epoch')\n",
    "        val_log = val_log_subset.groupby('epoch')[['coco/bbox_mAP']].mean().reset_index()\n",
    "    else:\n",
    "        print(\"--- GAGAL: Kolom 'step' atau 'coco/bbox_mAP' tidak ditemukan di log validasi.\")\n",
    "        return\n",
    "    # --- AKHIR PERBAIKAN ---\n",
    "    \n",
    "    # Gabungkan kedua log berdasarkan kolom 'epoch'\n",
    "    merged_log = pd.merge(train_log, val_log, on='epoch', how='inner')\n",
    "\n",
    "    if merged_log.empty:\n",
    "        print(\"--- GAGAL: Tidak dapat menggabungkan data training loss dan validation mAP. Periksa file log.\")\n",
    "        return\n",
    "\n",
    "    # Sisa fungsi untuk plotting tidak berubah\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Rata-rata Training Loss', color=color)\n",
    "    ax1.plot(merged_log['epoch'], merged_log['loss'], 'o-', color=color, label='Training Loss (rata-rata)')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation mAP', color=color)\n",
    "    ax2.plot(merged_log['epoch'], merged_log['coco/bbox_mAP'], 's-', color=color, label='Validation mAP')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Kurva Stabilitas Training: Loss vs. Validation mAP')\n",
    "    fig.tight_layout()\n",
    "    plot_path = os.path.join(report_dir, 'training_stability_curve.png')\n",
    "    plt.savefig(plot_path, dpi=200)\n",
    "    plt.show()\n",
    "    print(f\">>> Grafik stabilitas training disimpan di: {plot_path}\")\n",
    "\n",
    "# Panggil fungsi dengan variabel yang sudah didefinisikan di SEL 6\n",
    "create_stability_curve_by_epoch(latest_run_dir, REPORT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cb538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 8: FUNGSI EVALUASI KUSTOM (DENGAN PERBAIKAN TIPE DATA TENSOR)\n",
    "# ===================================================================\n",
    "\n",
    "from mmdet.registry import DATASETS\n",
    "\n",
    "def compute_iou_matrix(boxes1, boxes2):\n",
    "    # ... (fungsi ini tidak berubah)\n",
    "    if boxes1.size == 0 or boxes2.size == 0: return np.zeros((boxes1.shape[0], boxes2.shape[0]), dtype=np.float32)\n",
    "    x11, y11, x12, y12 = boxes1[:,0], boxes1[:,1], boxes1[:,2], boxes1[:,3]\n",
    "    x21, y21, x22, y22 = boxes2[:,0], boxes2[:,1], boxes2[:,2], boxes2[:,3]\n",
    "    inter_x1 = np.maximum(x11[:, None], x21[None, :]); inter_y1 = np.maximum(y11[:, None], y21[None, :])\n",
    "    inter_x2 = np.minimum(x12[:, None], x22[None, :]); inter_y2 = np.minimum(y12[:, None], y22[None, :])\n",
    "    inter_w = np.maximum(0, inter_x2 - inter_x1); inter_h = np.maximum(0, inter_y2 - inter_y1)\n",
    "    inter = inter_w * inter_h\n",
    "    area1 = (x12 - x11) * (y12 - y11); area2 = (x22 - x21) * (y22 - y21)\n",
    "    union = area1[:, None] + area2[None, :] - inter\n",
    "    return np.where(union > 0, inter / union, 0.0)\n",
    "\n",
    "def greedy_match(iou_mat, iou_thr=0.5):\n",
    "    # ... (fungsi ini tidak berubah)\n",
    "    matches = []; gt_used, pred_used = set(), set()\n",
    "    if iou_mat.size == 0: return matches\n",
    "    pairs = sorted([(i, j, iou_mat[i, j]) for i in range(iou_mat.shape[0]) for j in range(iou_mat.shape[1])], key=lambda x: x[2], reverse=True)\n",
    "    for i, j, iou in pairs:\n",
    "        if iou < iou_thr: break\n",
    "        if i in gt_used or j in pred_used: continue\n",
    "        gt_used.add(i); pred_used.add(j); matches.append((i, j))\n",
    "    return matches\n",
    "\n",
    "def evaluate_confusion_pr(model, dataset, output_dir, classes, device, score_thr=0.3, iou_thr=0.5):\n",
    "    print(f\"\\n>>> Memulai evaluasi kustom (Score Thr: {score_thr}, IoU Thr: {iou_thr})...\")\n",
    "    n_classes = len(classes)\n",
    "    conf_mat = np.zeros((n_classes, n_classes), dtype=np.int32)\n",
    "    per_class_counts = {'TP': np.zeros(n_classes, dtype=np.int32), 'FP': np.zeros(n_classes, dtype=np.int32), 'FN': np.zeros(n_classes, dtype=np.int32)}\n",
    "    pr_store = {c: {'scores': [], 'match': []} for c in range(n_classes)}\n",
    "\n",
    "    pipeline = Compose(cfg.val_dataloader.dataset.pipeline)\n",
    "\n",
    "    with open(filtered_valid_ann, 'r') as f: val_data = json.load(f)\n",
    "    img_id_to_anns = {}\n",
    "    for ann in val_data['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in img_id_to_anns: img_id_to_anns[img_id] = []\n",
    "        img_id_to_anns[img_id].append(ann)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for img_info in tqdm(val_data['images'], desc=\"Mengevaluasi gambar\"):\n",
    "          gt_anns = img_id_to_anns.get(img_info['id'], [])\n",
    "          gt_bboxes = np.array([ann['bbox'] for ann in gt_anns], dtype=np.float32)\n",
    "          if gt_bboxes.shape[0] > 0:\n",
    "              gt_bboxes[:, 2] += gt_bboxes[:, 0]\n",
    "              gt_bboxes[:, 3] += gt_bboxes[:, 1]\n",
    "          gt_labels = np.array([ann['category_id'] for ann in gt_anns], dtype=np.int64)\n",
    "\n",
    "          img_path = os.path.join(DATASET_ROOT, 'valid', img_info['file_name'])\n",
    "          data_info = {'img_path': img_path, 'img_id': 0}\n",
    "          processed_data = pipeline(data_info)\n",
    "          \n",
    "          input_tensor = processed_data['inputs']\n",
    "          \n",
    "          # --- PERBAIKAN UTAMA ADA DI SINI ---\n",
    "          # Konversi tipe data ke float, tambahkan dimensi batch, dan pindahkan ke device\n",
    "          batched_input = input_tensor.float().unsqueeze(0).to(device)\n",
    "          # --- AKHIR PERBAIKAN ---\n",
    "\n",
    "          data_samples_list = [processed_data['data_samples']]\n",
    "          results_list = model.predict(batched_input, data_samples_list)\n",
    "          \n",
    "          pred = results_list[0].pred_instances\n",
    "          keep = pred.scores >= score_thr\n",
    "          pred_bboxes, pred_scores, pred_labels = pred.bboxes[keep].cpu().numpy(), pred.scores[keep].cpu().numpy(), pred.labels[keep].cpu().numpy()\n",
    "\n",
    "          iou_mat = compute_iou_matrix(gt_bboxes, pred_bboxes)\n",
    "          matches = greedy_match(iou_mat, iou_thr=iou_thr)\n",
    "          matched_gt, matched_pred = {m[0] for m in matches}, {m[1] for m in matches}\n",
    "\n",
    "          for gi, pj in matches:\n",
    "              gt_c, pd_c = int(gt_labels[gi]), int(pred_labels[pj])\n",
    "              if gt_c == pd_c:\n",
    "                  per_class_counts['TP'][pd_c] += 1\n",
    "                  pr_store[pd_c]['scores'].append(float(pred_scores[pj]))\n",
    "                  pr_store[pd_c]['match'].append(1)\n",
    "              conf_mat[gt_c, pd_c] += 1\n",
    "\n",
    "          for j, pd_c_int in enumerate(pred_labels.astype(int)):\n",
    "              if j not in matched_pred:\n",
    "                  per_class_counts['FP'][pd_c_int] += 1\n",
    "                  pr_store[pd_c_int]['scores'].append(float(pred_scores[j]))\n",
    "                  pr_store[pd_c_int]['match'].append(0)\n",
    "\n",
    "          for i, gt_c_int in enumerate(gt_labels.astype(int)):\n",
    "              if i not in matched_gt:\n",
    "                  per_class_counts['FN'][gt_c_int] += 1\n",
    "    \n",
    "    # Sisa fungsi tidak berubah\n",
    "    eps = 1e-12\n",
    "    prec = per_class_counts['TP'] / (per_class_counts['TP'] + per_class_counts['FP'] + eps)\n",
    "    rec  = per_class_counts['TP'] / (per_class_counts['TP'] + per_class_counts['FN'] + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "    lines = [\"KELAS,TP,FP,FN,Precision,Recall,F1\"]\n",
    "    for c, name in enumerate(classes):\n",
    "        lines.append(f\"{name},{per_class_counts['TP'][c]},{per_class_counts['FP'][c]},{per_class_counts['FN'][c]},\"\n",
    "                     f\"{prec[c]:.4f},{rec[c]:.4f},{f1[c]:.4f}\")\n",
    "    per_class_txt_path = os.path.join(output_dir, \"per_class_metrics.csv\")\n",
    "    with open(per_class_txt_path, \"w\") as f: f.write(\"\\n\".join(lines))\n",
    "    print(f\">>> Metrik per kelas disimpan di: {per_class_txt_path}\")\n",
    "    fig_cm = plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='viridis', xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(f\"Confusion Matrix (IoU>{iou_thr}, Score>{score_thr})\"); plt.ylabel('True Label'); plt.xlabel('Predicted Label'); plt.tight_layout()\n",
    "    cm_path = os.path.join(output_dir, \"confusion_matrix.png\"); fig_cm.savefig(cm_path, dpi=200); plt.show()\n",
    "    print(f\">>> Confusion matrix disimpan di: {cm_path}\")\n",
    "    pr_curve_dir = os.path.join(output_dir, 'pr_curves')\n",
    "    os.makedirs(pr_curve_dir, exist_ok=True)\n",
    "    for c, class_name in enumerate(classes):\n",
    "        scores, match = np.array(pr_store[c]['scores']), np.array(pr_store[c]['match'])\n",
    "        if scores.size == 0: continue\n",
    "        order = np.argsort(-scores); scores, match = scores[order], match[order]\n",
    "        tp_cum, fp_cum = np.cumsum(match), np.cumsum(1 - match)\n",
    "        precision = tp_cum / np.maximum(tp_cum + fp_cum, 1)\n",
    "        total_pos = per_class_counts['TP'][c] + per_class_counts['FN'][c]\n",
    "        recall = tp_cum / max(total_pos, 1) if total_pos > 0 else np.zeros_like(tp_cum)\n",
    "        fig_pr = plt.figure(); plt.plot(recall, precision, '-'); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR Curve - {class_name}\"); plt.grid(); plt.xlim(-0.05, 1.05); plt.ylim(-0.05, 1.05)\n",
    "        pr_path = os.path.join(pr_curve_dir, f\"pr_curve_{c:02d}_{class_name.replace(' ','_')}.png\"); fig_pr.savefig(pr_path, dpi=200, bbox_inches='tight'); plt.close(fig_pr)\n",
    "    print(f\">>> PR curves disimpan di: {pr_curve_dir}\")\n",
    "\n",
    "if best_checkpoint_path:\n",
    "    model = MODELS.build(cfg.model)\n",
    "    checkpoint = torch.load(best_checkpoint_path, map_location='cpu', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    val_dataset = DATASETS.build(cfg.val_dataloader.dataset)\n",
    "    \n",
    "    evaluate_confusion_pr(model, val_dataset, REPORT_DIR, classes=CLASSES_TO_USE, device=device, score_thr=0.3, iou_thr=0.5)\n",
    "else:\n",
    "    print(\"\\nEvaluasi kustom dilewati karena tidak ada checkpoint terbaik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681af979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 9: INFERENSI PADA GAMBAR ACAK & RINGKASAN DETEKSI (FOLDER RAPI)\n",
    "# ===================================================================\n",
    "\n",
    "# Pastikan variabel latest_run_dir dan best_checkpoint_path dari SEL 6 sudah ada\n",
    "if 'latest_run_dir' in locals() and latest_run_dir and best_checkpoint_path:\n",
    "    print(\"\\n>>> Melakukan inferensi pada beberapa gambar validasi acak...\")\n",
    "    \n",
    "    # --- PERBAIKAN: Buat folder inferensi di dalam direktori run terbaru ---\n",
    "    INFERENCE_DIR = os.path.join(latest_run_dir, 'inference_results')\n",
    "    os.makedirs(INFERENCE_DIR, exist_ok=True)\n",
    "    print(f\"Hasil inferensi akan disimpan di: {INFERENCE_DIR}\")\n",
    "    \n",
    "    visualizer = DetLocalVisualizer(vis_backends=[dict(type='LocalVisBackend')], name='visualizer')\n",
    "    visualizer.dataset_meta = cfg.val_dataloader.dataset.metainfo\n",
    "\n",
    "    with open(filtered_valid_ann, 'r') as f: val_data = json.load(f)\n",
    "    num_samples = min(5, len(val_data['images']))\n",
    "    random_samples = random.sample(val_data['images'], k=num_samples)\n",
    "\n",
    "    # Definisikan device dan pindahkan model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, img_info in enumerate(random_samples):\n",
    "            img_path = os.path.join(DATASET_ROOT, 'valid', img_info['file_name'])\n",
    "            original_image = mmcv.imread(img_path)\n",
    "            \n",
    "            # Siapkan data untuk inferensi manual\n",
    "            pipeline = Compose(test_pipeline)\n",
    "            data_info = {'img_path': img_path, 'img_id': 0}\n",
    "            processed_data = pipeline(data_info)\n",
    "            \n",
    "            batched_input = processed_data['inputs'].float().unsqueeze(0).to(device)\n",
    "            data_samples_list = [processed_data['data_samples']]\n",
    "\n",
    "            results_list = model.predict(batched_input, data_samples_list)\n",
    "            \n",
    "            # Hitung Ringkasan\n",
    "            pred_instances = results_list[0].pred_instances\n",
    "            score_thr = 0.3 # Anda bisa menaikkan ini jika terlalu banyak deteksi\n",
    "            keep_indices = pred_instances.scores >= score_thr\n",
    "            labels_kept = pred_instances.labels[keep_indices].cpu().numpy()\n",
    "            class_names = visualizer.dataset_meta['classes']\n",
    "            counts = {name: int((labels_kept == i).sum()) for i, name in enumerate(class_names)}\n",
    "            total_detected = int(keep_indices.sum())\n",
    "\n",
    "            # Simpan Ringkasan ke Teks\n",
    "            summary_txt_path = os.path.join(INFERENCE_DIR, f\"summary_img_{img_info['id']}.txt\")\n",
    "            with open(summary_txt_path, \"w\") as f:\n",
    "                f.write(f\"Hasil Deteksi pada: {os.path.basename(img_path)}\\n\")\n",
    "                f.write(f\"Score Threshold: {score_thr}\\n\" + \"=\"*30 + \"\\n\")\n",
    "                for class_name, count in counts.items():\n",
    "                    if count > 0: f.write(f\"{class_name}: {count}\\n\")\n",
    "                f.write(\"=\"*30 + f\"\\nTOTAL TERDETEKSI: {total_detected}\\n\")\n",
    "            print(f\"Ringkasan deteksi disimpan di: {summary_txt_path}\")\n",
    "\n",
    "            # Simpan Visualisasi\n",
    "            output_filename = f\"visualization_img_{img_info['id']}.jpg\"\n",
    "            output_path = os.path.join(INFERENCE_DIR, output_filename)\n",
    "            visualizer.add_datasample(\n",
    "                name=f'prediction_{i}', image=original_image,\n",
    "                data_sample=results_list[0], draw_gt=False,\n",
    "                out_file=output_path, pred_score_thr=score_thr)\n",
    "            print(f\"Visualisasi disimpan di: {output_path}\\n\")\n",
    "\n",
    "    print(\"\\nProses visualisasi selesai.\")\n",
    "else:\n",
    "    print(\"\\nInferensi dilewati karena tidak ada direktori run/checkpoint yang valid.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f374a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeebfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29965d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abdaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# SEL 4: PEMBUATAN SKRIP train.py (TANPA LR SCHEDULER)\n",
    "# ===================================================================\n",
    "\n",
    "script_content = r\"\"\"\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' \n",
    "import sys, json, torch, numpy as np, pandas as pd, gc, argparse\n",
    "import torchvision\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "import torch.utils.data\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "import utils\n",
    "from engine_robust import train_one_epoch, evaluate_robust as evaluate\n",
    "import transforms as T \n",
    "\n",
    "# ===================================================================\n",
    "# KELAS DAN FUNGSI BASELINE ASLI ANDA\n",
    "# ===================================================================\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    transforms.append(T.ToTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "class BreadDataset(CocoDetection):\n",
    "    # ... (Isi class BreadDataset sama persis seperti versi baseline) ...\n",
    "    def __init__(self, root, annFile, transform=None):\n",
    "        super(BreadDataset, self).__init__(root, annFile)\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, idx):\n",
    "        img, target_list = super(BreadDataset, self).__getitem__(idx)\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "        image_id = torch.tensor([self.ids[idx]])\n",
    "        if len(target_list) > 0:\n",
    "            for ann in target_list:\n",
    "                x, y, w, h = ann['bbox']\n",
    "                if w > 0 and h > 0:\n",
    "                    boxes.append([x, y, x + w, y + h])\n",
    "                    labels.append(ann['category_id'])\n",
    "                    areas.append(ann['area'])\n",
    "                    iscrowd.append(ann['iscrowd'])\n",
    "        target = {\"image_id\": image_id}\n",
    "        if len(boxes) > 0:\n",
    "            target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            target[\"area\"] = torch.as_tensor(areas, dtype=torch.float32)\n",
    "            target[\"iscrowd\"] = torch.as_tensor(iscrowd, dtype=torch.int64)\n",
    "        else:\n",
    "            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target[\"labels\"] = torch.zeros(0, dtype=torch.int64)\n",
    "            target[\"area\"] = torch.zeros(0, dtype=torch.float32)\n",
    "            target[\"iscrowd\"] = torch.zeros(0, dtype=torch.int64)\n",
    "        if self.transform is not None:\n",
    "            img, target = self.transform(img, target)\n",
    "        return img, target\n",
    "\n",
    "def freeze_bn(module):\n",
    "    if isinstance(module, torch.nn.BatchNorm2d):\n",
    "        module.eval()\n",
    "        if module.weight is not None: module.weight.requires_grad = False\n",
    "        if module.bias is not None: module.bias.requires_grad = False\n",
    "\n",
    "def main(args):\n",
    "    model_base_name = args.model_base_name\n",
    "    lr_str = str(args.lr).replace('.', '_')\n",
    "    PROJECT_NAME = f\"{model_base_name}_e{args.epochs}_bs{args.batch_size}_lr{lr_str}\"\n",
    "    DATASET_ROOT = 'dataset'\n",
    "    CLASSES_TO_USE = ['baguette', 'cornbread', 'croissant', 'ensaymada', 'flatbread', 'sourdough', 'wheat-bread', 'white-bread', 'whole-grain-bread', 'pandesal']\n",
    "    NUM_CLASSES = len(CLASSES_TO_USE)\n",
    "    WORK_DIR = f\"outputs/{PROJECT_NAME}\"\n",
    "    os.makedirs(WORK_DIR, exist_ok=True)\n",
    "    print(\"=\"*60 + f\"\\nMemulai Eksperimen: {PROJECT_NAME}\\n\" + \"=\"*60)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    dataset = BreadDataset(os.path.join(DATASET_ROOT, 'train'), os.path.join(DATASET_ROOT, 'train', 'filtered_annotations.coco.json'), transform=get_transform(train=True))\n",
    "    dataset_test = BreadDataset(os.path.join(DATASET_ROOT, 'valid'), os.path.join(DATASET_ROOT, 'valid', 'filtered_annotations.coco.json'), transform=get_transform(train=False))\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=2, collate_fn=utils.collate_fn, drop_last=True)\n",
    "    data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=2, collate_fn=utils.collate_fn)\n",
    "    \n",
    "    model = ssdlite320_mobilenet_v3_large(weights='DEFAULT')\n",
    "    num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "    in_channels = [m[0][0].in_channels for m in model.head.classification_head.module_list]\n",
    "    new_head = SSDLiteClassificationHead(in_channels=in_channels, num_anchors=num_anchors, num_classes=(NUM_CLASSES + 1), norm_layer=torch.nn.BatchNorm2d)\n",
    "    model.head.classification_head = new_head\n",
    "    model.to(device)\n",
    "    model.apply(freeze_bn)\n",
    "    \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=args.lr, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # --- PERUBAHAN 1: HAPUS BARIS INI ---\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=0)\n",
    "    \n",
    "    print(f\"--- MEMULAI TRAINING {args.epochs} EPOCH (TANPA LR SCHEDULER) ---\")\n",
    "    best_map = 0.0\n",
    "    training_history = []\n",
    "    for epoch in range(args.epochs):\n",
    "        metric_logger = train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
    "        eval_result = evaluate(model, data_loader_test, device=device)\n",
    "        current_map = eval_result.coco_eval['bbox'].stats[0]\n",
    "        training_history.append({'epoch': epoch + 1, 'loss': metric_logger.meters['loss'].global_avg, 'mAP_0.50:0.95': current_map, 'lr': optimizer.param_groups[0][\"lr\"]})\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs}: Avg Loss={metric_logger.meters['loss'].global_avg:.4f}, mAP={current_map:.4f}\")\n",
    "        \n",
    "        # --- PERUBAHAN 2: HAPUS BARIS INI ---\n",
    "        # lr_scheduler.step()\n",
    "        \n",
    "        if current_map > best_map:\n",
    "            best_map = current_map\n",
    "            utils.save_on_master(model.state_dict(), os.path.join(WORK_DIR, 'best_model.pth'))\n",
    "            print(f\"*** Best mAP updated: {best_map:.4f} (model saved) ***\")\n",
    "    \n",
    "    print(f\"\\n--- TRAINING SELESAI --- Best mAP: {best_map:.4f}\")\n",
    "    pd.DataFrame(training_history).to_csv(os.path.join(WORK_DIR, 'training_log.csv'), index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--model-base-name', type=str, default=\"ssdmobilenetv3\")\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--batch-size', type=int, default=4)\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open('train.py', 'w', encoding='utf-8') as f: f.write(script_content.strip())\n",
    "    print(\">>> Skrip 'train.py' (tanpa scheduler) berhasil dibuat.\")\n",
    "except Exception as e:\n",
    "    print(f\">>> GAGAL membuat skrip 'train.py': {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
